{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705df3a7-3270-4fee-948c-c94c75e8a208",
   "metadata": {},
   "source": [
    "# Demo: Apply TissueFormer to user-provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e843c4c-a802-4fef-ba45-1dcd60e901ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_home/wuqitian/anaconda3/envs/bio/lib/python3.12/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /local_home/wuqitian/anaconda3/envs/bio/lib/python3.12/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/local_home/wuqitian/anaconda3/envs/bio/lib/python3.12/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /local_home/wuqitian/anaconda3/envs/bio/lib/python3.12/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "parent_dir = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from run_finetune import run_update, run_train, run_test, evaluate\n",
    "from parse import parse_pretrain_method, parse_regression_method, parse_classification_method, parser_add_main_args\n",
    "from utils import dataset_create, dataset_create_split, k_shot_split\n",
    "from dataloader import CustomDataset\n",
    "\n",
    "from torch_cluster import knn_graph\n",
    "import scanpy as sc\n",
    "\n",
    "import warnings\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c145dc6-287a-4bcb-95eb-015f650159fc",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Here we use the lung fibrosis data as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bc4ed1c-14f8-4994-9155-eab57f1ad788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading dataset: 100%|################################################| 6/6 [00:12<00:00,  2.06s/it]\n",
      "loading dataset: 100%|################################################| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_load(dir_path, sample):\n",
    "    file_path = os.path.join(dir_path, sample) + '.h5ad'\n",
    "    adata = sc.read(file_path)\n",
    "    X_log1p = adata.layers['X_log1p']\n",
    "\n",
    "    gene_mask = adata.var['gene_filter_mask']\n",
    "    cell_by_gene = X_log1p[:, gene_mask]\n",
    "    gene_index = adata.var['gene_filtered_idx'][gene_mask]\n",
    "\n",
    "    cell_image_emb = adata.obsm['embeddings']\n",
    "    cell_location = adata.obsm['centroids']\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['x'] = torch.tensor(cell_image_emb, dtype=torch.float)\n",
    "    dataset['y'] = torch.tensor(cell_by_gene, dtype=torch.float)\n",
    "    dataset['gene_idx'] = torch.tensor(gene_index, dtype=torch.long)\n",
    "    dataset['edge_index'] = knn_graph(torch.tensor(cell_location, dtype=torch.float), k=5, loop=False)\n",
    "\n",
    "    hvg_gene_rank = adata.var['highly_variable_rank'][gene_mask]\n",
    "    dataset['hvg_gene_rank'] = torch.tensor(hvg_gene_rank, dtype=torch.long)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def dataset_create(dir_path, samples):\n",
    "    datasets = []\n",
    "    pbar = tqdm(samples, desc='loading dataset', ncols=100, ascii=True)\n",
    "    for s in pbar:\n",
    "        dataset = data_load(dir_path, s)\n",
    "        idx = torch.arange(dataset['x'].shape[0])\n",
    "        datasets.append(dataset)\n",
    "        pbar.clear()\n",
    "        pbar.refresh()\n",
    "    return CustomDataset(datasets)\n",
    "\n",
    "dir_path = '/ewsc/wuqitian/lung_preprocess'\n",
    "meta_info = pd.read_csv(\"../../data/meta_info_lung.csv\")\n",
    "\n",
    "# train data can be used as the reference for in-context learning or for finetuning the model\n",
    "train_samples = meta_info[meta_info['affect'] == 'Unaffected']['sample'].tolist()[:-1]\n",
    "\n",
    "# test data for evaluation\n",
    "test_samples = meta_info[meta_info['affect'] == 'Unaffected']['sample'].tolist()[-1:]\n",
    "\n",
    "# create dataloader\n",
    "train_datasets = dataset_create(dir_path, train_samples)\n",
    "train_dataloader = DataLoader(train_datasets, batch_size=1, shuffle=True)\n",
    "test_datasets = dataset_create(dir_path, test_samples)\n",
    "test_dataloader = DataLoader(test_datasets, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0340a95-0e55-4c73-8c3e-9bc333b906d3",
   "metadata": {},
   "source": [
    "## Model preparation: load pretrained checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eea465e-b581-4da8-baf4-63f27d0997da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models.regression as regression\n",
    "import encoders\n",
    "\n",
    "device = torch.device(\"cuda:0\") # torch.device(\"cpu\")\n",
    "encoder1 = encoders.Transformer(in_channels=1536, hidden_channels=1024, num_layers_prop=2, num_layers_mlp=2, num_attn_heads=1,\n",
    "                                        dropout=0., use_bn=True, use_graph=True, use_residual=True).to(device)\n",
    "model_ours = regression.InContext_Predict(encoder1, hidden_channels=1024, out_channels=340, batch_size=100,\n",
    "                                    num_neighbors=1000, device=device).to(device)\n",
    "    \n",
    "pretrained_state_dict = torch.load('../../model_checkpoints/ours_pretrain_xenium_lung.pth') # can specify the model versions\n",
    "encoder1_pretrained_dict = {k: v for k, v in pretrained_state_dict.items() if k.startswith(\"encoder1.\")}\n",
    "model_state_dict = model_ours.state_dict()\n",
    "encoder1_model_dict = {k: v for k, v in model_state_dict.items() if k.startswith(\"encoder1.\")}\n",
    "for k, v in encoder1_pretrained_dict.items():\n",
    "    assert (k in encoder1_model_dict)\n",
    "    assert (v.size() == encoder1_model_dict[k].size())\n",
    "model_state_dict.update(encoder1_pretrained_dict)\n",
    "model_ours.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845d2db-d869-45c2-a3ac-46558c1c38c5",
   "metadata": {},
   "source": [
    "## Predict gene expression from histology images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26bb696c-30a0-465f-b39c-613b6769a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading reference data finished\n",
      "prediction for sample 0 finished\n",
      "(12870, 340)\n",
      "(12870, 340)\n"
     ]
    }
   ],
   "source": [
    "def update(model, dataloader, device, use_gene_idx=True):\n",
    "    model.eval()\n",
    "    for dataset in dataloader:\n",
    "        data = dataset[0]\n",
    "        inputs, labels = data.x.to(device), data.y.to(device)\n",
    "        edge_index = data.edge_index.to(device)\n",
    "        if use_gene_idx:\n",
    "            gene_idx = data.gene_idx.to(device)\n",
    "        else:\n",
    "            gene_idx = None\n",
    "        train_idx = torch.arange(labels.shape[0])\n",
    "\n",
    "        model.update(inputs, labels, gene_idx, train_idx, edge_index)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, data, device, use_gene_idx=True):\n",
    "    model.eval()\n",
    "\n",
    "    inputs, labels = data.x.to(device), data.y.to(device)\n",
    "    edge_index = data.edge_index.to(device)\n",
    "    if use_gene_idx:\n",
    "        gene_idx = data.gene_idx.to(device)\n",
    "    else:\n",
    "        gene_idx = None\n",
    "\n",
    "    # task is gene expression prediction\n",
    "    outputs = model(inputs, gene_idx, edge_index)\n",
    "\n",
    "    return labels.cpu().numpy(), outputs.cpu().numpy()\n",
    "\n",
    "# load reference data into the model for in-context learning prediction\n",
    "update(model_ours, train_dataloader, device, use_gene_idx=False)\n",
    "print(\"loading reference data finished\")\n",
    "\n",
    "# predict gene expression for each test slide\n",
    "for i, dataset in enumerate(test_dataloader):\n",
    "    data = dataset[0]\n",
    "    y_true, y_pred = predict(model_ours, data, device, use_gene_idx=False)\n",
    "    print(f\"prediction for sample {i} finished\")\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0220006-5bb8-4851-8a40-0f5398e29041",
   "metadata": {},
   "source": [
    "## Extract cell embeddings and cell-cell attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89b19ec1-4d8e-474a-9bd5-6f742c08d304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 12870, 1024)\n",
      "[[[ 1.3715379e+00  1.4454957e+00 -7.5154972e-01 ... -7.6502818e-01\n",
      "   -7.3900068e-01 -7.2247493e-01]\n",
      "  [ 9.2117399e-01  1.1292846e+00 -6.5943491e-01 ... -6.6768020e-01\n",
      "   -6.6846138e-01 -6.4686137e-01]\n",
      "  [ 1.7809750e+00  1.5489399e+00 -7.9155129e-01 ... -8.0272007e-01\n",
      "   -8.1195861e-01 -7.8880268e-01]\n",
      "  ...\n",
      "  [-6.4784706e-01 -5.3436673e-01  7.5236696e-01 ...  7.0106542e-01\n",
      "    7.0526987e-01  8.0448413e-01]\n",
      "  [-6.2402189e-01 -4.1940057e-01  5.1424354e-01 ...  4.7407335e-01\n",
      "    5.0584358e-01  8.2884425e-01]\n",
      "  [ 1.6685019e+00  1.8540162e+00 -8.1977361e-01 ... -8.1474954e-01\n",
      "   -8.3234191e-01 -8.5177535e-01]]\n",
      "\n",
      " [[-4.8717000e-02  2.1894351e-01 -1.6253795e-01 ... -5.8698375e-02\n",
      "   -4.6126145e-01 -4.6564618e-01]\n",
      "  [-2.2871874e-01  2.4311823e-01 -1.6629460e-01 ... -7.7653445e-02\n",
      "   -4.2097694e-01 -3.9412290e-01]\n",
      "  [ 2.2985333e-01  1.8899480e-01 -2.4468711e-01 ...  6.5168940e-02\n",
      "   -4.6890473e-01 -4.5090723e-01]\n",
      "  ...\n",
      "  [-6.9585079e-01 -5.0514692e-01 -5.6706842e-02 ... -2.7636921e-01\n",
      "    3.4921166e-02 -1.2750870e-01]\n",
      "  [-7.0752680e-01 -5.2221316e-01 -2.2896361e-01 ... -3.4881768e-01\n",
      "   -6.8374604e-02 -5.9575897e-02]\n",
      "  [ 8.2737252e-02  1.6753490e-01 -2.4124055e-01 ... -2.7481758e-04\n",
      "   -5.1192313e-01 -5.4957938e-01]]\n",
      "\n",
      " [[-7.7144778e-01 -8.6829787e-01  5.3254592e-01 ...  1.3945187e+00\n",
      "    1.0577174e+00  6.4510655e-01]\n",
      "  [-7.9186726e-01 -8.6621988e-01  5.2157402e-01 ...  1.3902776e+00\n",
      "    1.0647744e+00  6.6795033e-01]\n",
      "  [-7.4869746e-01 -8.6712915e-01  4.8746118e-01 ...  1.5081841e+00\n",
      "    1.0638701e+00  6.3767141e-01]\n",
      "  ...\n",
      "  [-6.6891909e-01 -7.9789120e-01  5.8360785e-01 ...  6.1622024e-01\n",
      "    8.9361727e-01  7.5822997e-01]\n",
      "  [-6.8432480e-01 -8.0755949e-01  4.4127670e-01 ...  6.1880612e-01\n",
      "    8.1331801e-01  8.1471866e-01]\n",
      "  [-7.6205087e-01 -8.7077546e-01  5.0022006e-01 ...  1.3935956e+00\n",
      "    1.1067126e+00  5.9905541e-01]]]\n",
      "(2, 12870, 12870)\n",
      "[[[-0.41585657 -0.41698685 -0.4177317  ...  0.42954674  0.42826495\n",
      "   -0.41444975]\n",
      "  [-0.48781803 -0.48948556 -0.488548   ...  0.4938018   0.49264005\n",
      "   -0.48661605]\n",
      "  [-0.48470446 -0.4864887  -0.48524252 ...  0.4964047   0.49584717\n",
      "   -0.48222744]\n",
      "  ...\n",
      "  [ 0.04199612  0.04842915  0.04044331 ... -0.09305693 -0.09657548\n",
      "    0.0360539 ]\n",
      "  [-0.18988435 -0.18420187 -0.19065773 ...  0.13839495  0.13075472\n",
      "   -0.19531882]\n",
      "  [-0.30899656 -0.31095347 -0.30952832 ...  0.33156624  0.33287036\n",
      "   -0.30559415]]\n",
      "\n",
      " [[ 0.40738368  0.3794868   0.41619006 ... -0.03581262 -0.0268977\n",
      "    0.4234224 ]\n",
      "  [ 0.4030621   0.3755742   0.4126531  ... -0.0399408  -0.03121598\n",
      "    0.4187893 ]\n",
      "  [ 0.39366123  0.3660456   0.40169397 ... -0.03566909 -0.02770182\n",
      "    0.4094414 ]\n",
      "  ...\n",
      "  [ 0.49331185  0.47277334  0.4964172  ...  0.09754495  0.11705925\n",
      "    0.50430006]\n",
      "  [ 0.49228477  0.47048095  0.49544722 ...  0.09507962  0.11262209\n",
      "    0.50434244]\n",
      "  [ 0.4120145   0.38453344  0.42087033 ... -0.03408675 -0.02406074\n",
      "    0.42761487]]]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_embs_attn(model, data, device):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = data.x.to(device)\n",
    "    edge_index = data.edge_index.to(device)\n",
    "\n",
    "    embs = model_ours.encoder1.get_embeddings(inputs, edge_index).cpu().numpy() # [layer num, cell num, hidden size]\n",
    "    attn_maps = model_ours.encoder1.get_attentions(inputs, edge_index).cpu().numpy() # [layer num, cell num, cell num]\n",
    "\n",
    "    return embs, attn_maps \n",
    "\n",
    "# get embeddings and attention maps for each slide\n",
    "for i, dataset in enumerate(test_dataloader):\n",
    "    data = dataset[0]\n",
    "    embs, attn_maps = get_embs_attn(model_ours, data, device)\n",
    "    print(embs.shape)\n",
    "    print(embs)\n",
    "    print(attn_maps.shape)\n",
    "    print(attn_maps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575bff8b-7be2-4dbc-bc2e-9654f563ef80",
   "metadata": {},
   "source": [
    "## Finetune the model on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86ca0724-81d8-4bb5-b9ae-7cc881aab60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: -5.3346\n",
      "Epoch [2/50], Loss: -5.3493\n",
      "Epoch [3/50], Loss: -5.3589\n",
      "Epoch [4/50], Loss: -5.3656\n",
      "Epoch [5/50], Loss: -5.3722\n",
      "Epoch [6/50], Loss: -5.3781\n",
      "Epoch [7/50], Loss: -5.3848\n",
      "Epoch [8/50], Loss: -5.3904\n",
      "Epoch [9/50], Loss: -5.3966\n",
      "Epoch [10/50], Loss: -5.4022\n",
      "Epoch [11/50], Loss: -5.4073\n",
      "Epoch [12/50], Loss: -5.4117\n",
      "Epoch [13/50], Loss: -5.4131\n",
      "Epoch [14/50], Loss: -5.4132\n",
      "Epoch [15/50], Loss: -5.4165\n",
      "Epoch [16/50], Loss: -5.4155\n",
      "Epoch [17/50], Loss: -5.4245\n",
      "Epoch [18/50], Loss: -5.4276\n",
      "Epoch [19/50], Loss: -5.4311\n",
      "Epoch [20/50], Loss: -5.4335\n",
      "Epoch [21/50], Loss: -5.4345\n",
      "Epoch [22/50], Loss: -5.4409\n",
      "Epoch [23/50], Loss: -5.4424\n",
      "Epoch [24/50], Loss: -5.4431\n",
      "Epoch [25/50], Loss: -5.4482\n",
      "Epoch [26/50], Loss: -5.4485\n",
      "Epoch [27/50], Loss: -5.4530\n",
      "Epoch [28/50], Loss: -5.4558\n",
      "Epoch [29/50], Loss: -5.4577\n",
      "Epoch [30/50], Loss: -5.4621\n",
      "Epoch [31/50], Loss: -5.4632\n",
      "Epoch [32/50], Loss: -5.4624\n",
      "Epoch [33/50], Loss: -5.4671\n",
      "Epoch [34/50], Loss: -5.4687\n",
      "Epoch [35/50], Loss: -5.4645\n",
      "Epoch [36/50], Loss: -5.4677\n",
      "Epoch [37/50], Loss: -5.4715\n",
      "Epoch [38/50], Loss: -5.4681\n",
      "Epoch [39/50], Loss: -5.4764\n",
      "Epoch [40/50], Loss: -5.4815\n",
      "Epoch [41/50], Loss: -5.4824\n",
      "Epoch [42/50], Loss: -5.4865\n",
      "Epoch [43/50], Loss: -5.4857\n",
      "Epoch [44/50], Loss: -5.4851\n",
      "Epoch [45/50], Loss: -5.4788\n",
      "Epoch [46/50], Loss: -5.4845\n",
      "Epoch [47/50], Loss: -5.4898\n",
      "Epoch [48/50], Loss: -5.4918\n",
      "Epoch [49/50], Loss: -5.4923\n",
      "Epoch [50/50], Loss: -5.4954\n"
     ]
    }
   ],
   "source": [
    "from models.pretrain import Model_Pretrain\n",
    "\n",
    "def train(model, dataloader, optimizer, device, accumulate_steps=1):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, dataset in enumerate(dataloader):\n",
    "        data = dataset[0]\n",
    "        x1, x2 = data.x.to(device), data.y.to(device)\n",
    "        edge_index = data.edge_index.to(device)\n",
    "        gene_idx = data.gene_idx.to(device)\n",
    "        train_idx = torch.arange(x1.shape[0]).to(device)\n",
    "\n",
    "        loss = model.loss(x1, x2, gene_idx, train_idx, edge_index)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % accumulate_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# load pretrain model\n",
    "encoder1 = encoders.Transformer(in_channels=1536, hidden_channels=1024, num_layers_prop=2, num_layers_mlp=2, num_attn_heads=1,\n",
    "                                        dropout=0., use_bn=True, use_graph=True, use_residual=True).to(device)\n",
    "encoder2 = encoders.MLP(in_channels=256, hidden_channels=1024, num_layers=1, dropout=0.).to(device)\n",
    "gene_embeddings = torch.zeros((23258, 256), dtype=torch.float).to(device)\n",
    "model_pretrain = Model_Pretrain(encoder1, encoder2, gene_embeddings, reg_w=0.5, ge_trainable=True, ge_pretrained=False).to(device)\n",
    "pretrained_state_dict = torch.load('../../model_checkpoints/ours_pretrain_visium_all.pth')\n",
    "model_pretrain.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_pretrain.parameters()), lr=1e-5,\n",
    "                       weight_decay=0.)\n",
    "\n",
    "# finetune the model\n",
    "epoch_nums = 50\n",
    "for epoch in range(epoch_nums):\n",
    "    train_loss = train(model_pretrain, train_dataloader, optimizer, device)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{epoch_nums}], Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4baca2-90cb-45d3-9b6f-9323f07bc5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "bio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
